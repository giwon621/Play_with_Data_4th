{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM\n",
    "\n",
    "- 선형이나 비선형 분류 : ok\n",
    "- regression : ok\n",
    "- anomaly detection : ok\n",
    "- large data set(especially column wise) : no...\n",
    "\n",
    "### 어떤 원리인건데 그럼??\n",
    "\n",
    "- Maximal Margin Classfier의 일반화된 모델\n",
    "- ???쟤는 또 뭐야???\n",
    "----------\n",
    "- *한정된 시간으로 모든 내용을 다루기는 힘들 수 있지만 일단 스터디 내용은 업로드 합니다. 스터디 발표에서는 책에 나온 내용 위주 & 실제 코드 활용을 통해 진행하려합니다.\n",
    "\n",
    "\n",
    "### Content\n",
    "\n",
    "##### 1. MMC(Maximal Margin Classifier)\n",
    "##### 2. SVC(Support Vector Classifier)\n",
    "##### 3. SVM(Support Vector Machine)\n",
    "##### 4. Extension of SVM(more than two classes)\n",
    "##### 5. Relation between SVM and other statistical model\n",
    "-------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. MMC\n",
    "\n",
    "* Hyperplane\n",
    ">- *def<br />*\n",
    ">a flat affine subspace of hyperplane dimension p − 1.<br /><br />\n",
    ">- ex)<br />\n",
    ">2차원 평면에서는 선이 hyperplane<br /><br />\n",
    ">β0 + β1X1 + β2X2 = 0<br /><br />\n",
    ">로 정의할 수 있음<br /><br />\n",
    "\n",
    "p-dimensional 까지 확장하면...<br /><br />\n",
    "β0 + β1X1 + β2X2 + ... + βpXp = 0  --- (a)<br/><br/>\n",
    "만약 어떤 점 X = (X1, X2,...,Xp)^T 가 식(a)를 만족하면 점 X는 (a) Hyperplane위에 있다.\n",
    "\n",
    "![Alt text](sc01.png)\n",
    "\n",
    "* Hyperplane을 이용해서 분류하기\n",
    "Hyperplane을 이용해서 분류하기 위해 먼저 행렬X(n개의 observations by p개의 dimension)를 정의<br/>\n",
    "\n",
    "![Alt text](sc02.png)\n",
    "\n",
    "위 그룹과 y값 2개로 나뉠수음 train/test 셋으로 나눌수 있기 때문에 lr, tree등 접근 방식이 가능 하지만 여기서는 'separating hyperplane'개념으로 접근\n",
    "\n",
    "#### hyperplane을 통해 관측값들을 그들의 레이블값(y)으로 완벽하게 나눌 수 있다는 컨셉에서 시작\n",
    "\n",
    "![Alt text](sc03.png)\n",
    "\n",
    "이때 각각의 y값에 대해서\n",
    "\n",
    "![Alt text](sc04.png)\n",
    "\n",
    "* The Maximal Margin Classifier\n",
    "\n",
    "#### hyperplane으로 분류를 한다는 것은 알겠는데 그럼 hyperplane은 어떻게 만드나요?\n",
    "\n",
    "-> 경계(margin)간의 거리가 최대(maximal)가 되는 hyperplane을 만들자\n",
    "\n",
    "![Alt text](sc05.png)\n",
    "\n",
    "그럼을 보면 가운데 직선이 maximal margin hyperplane이고 margin은 점선과 직선사이의 수직 거리이다.<br/>\n",
    "margin(점선)은 MMH(직선)의 절편 이동에 의해 만들어지는데 가장 먼저 접하는 관측치까지 이동한다. <br/><br/>\n",
    "이 때, 저 관측치들을 Support Vector라고 한다.(margin 결정을 'support'함 & p-차원 공간 상의 벡터)\n",
    "\n",
    "#### 잘 생각해보면 위 그림에서 margin은 단 3개의 관측치에 의해 결정되는데, 이에 관해서는 나중에 다루어짐\n",
    "\n",
    "* Construction of the Maximal Margin Classifier\n",
    "개념적인 측면에 벗어나서 이제 어떻게 margin을 maximize할 지 최적화 문제로 넘어가자\n",
    "\n",
    "![Alt text](sc06.png)\n",
    "다음 3개의 제약식을 만족하는 최적화 문제\n",
    "즉 M을 값을 최대화하기 위한 β0,...,βp를 찾는 풀이법은 생략...<br/>\n",
    "\n",
    ">#### 한계점 & 해결책\n",
    ">separating hyperplane이 없는 경우 M>0 조건에서 해를 구할 수 없는 경우<br/>\n",
    "![Alt text](sc06.png)\n",
    "위의 그림에서 완벽하게 모든 클래스별로 나누는 hyperplane은 없음...<br/>\n",
    "이럴때는 몇개의 관측치가 error로 들어가더라도 분리해낼 수 있는 모델이 필요하다<br/><br/>\n",
    "-> soft margin이 등장 <br/><br/>\n",
    "MMC의 일반화 형태인 Support vector classifier로 non-separable case를 해결\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "### 2. SVC\n",
    "- Overview of the Support Vector Machine\n",
    "기존 MMC 의 경우 두 레이블을 완벽하게 분리해야해서 train data에 대해 심한 overfit이 발생할 가능성이 높다.\n",
    "![Alt text](sc07.png)\n",
    "위의 그림의 왼쪽 MMC에서 단 하나의 점이 추가되었을 뿐인데 hyperplane은 급격하게 변하고 margin도 낮아지는 것을 볼 수 있다.<br/>\n",
    "따라서 새로운 hyperplane은...<br/><br/>\n",
    "#### 개별 관측치들에 대한 강건함(robustness) <- 좀더 일반화(Generalize)할 수 있는 모델\n",
    "#### Not Perfect, But Most <- 대부분의 데이터에 대한 효과적인 분류<br/>\n",
    "-> support vector classifier를 통해 trade-off 문제에 접근<br/><br/>\n",
    "- Details of the Support Vector Classifier\n",
    "\n",
    "![Alt text](sc08.png)\n",
    "\n",
    "이전 3개의 제약식에 새로운 조건이 추가된 형태. C는 음이 아닌 정수로 svm의 튜닝 파라미터이고 ε값은 margin에서 잘못 분류된 각각의 관측치들을 어느정도 허용하기 위해 넣은 slack variable.<br/>\n",
    "3번째식을 보면 ε값이 1을 넘기면 오분류되는 케이스가 발생한다.\n",
    "<br/><br/>\n",
    "C는 ε의 합계로 margin이 얼마나 tolerate한지 결정. 즉 C=0인 경우는 maximal margin hyperplane 최적화와 유사함\n",
    "\n",
    "-> C는 모델의 유연함?을 결정하는 파라미터로 C가 높을수록 margin이 넓어지고 많은 관측치들이 margin내부를 침범함\n",
    "\n",
    "![Alt text](sc09.png)\n",
    "\n",
    "왼쪽 위의 그래프에서 C가 가장 높고 점점 낮추면 다음과 같이 margin이 줄어드는 것을 확인할 수 있다\n",
    "\n",
    "---\n",
    "### 3. SVM\n",
    "지금까지 선형 classifier에 대해 알아보았다. 이제 이를 비선형 결정경계로 전환해야한다!\n",
    "왜냐고??\n",
    "![Alt text](sc10.png)\n",
    "클래스가 2개의 구역으로 분리된 경우 이와 같이 linear classifier는 정상적으로 작동하지 못하기때문\n",
    "\n",
    "- Classification with Non-linear Decision Boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ref\n",
    "- An intoduction to statistical learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
