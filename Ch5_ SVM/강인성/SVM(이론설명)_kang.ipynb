{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM\n",
    "\n",
    "- 선형이나 비선형 분류 : ok\n",
    "- regression : ok\n",
    "- anomaly detection : ok\n",
    "- large data set(especially column wise) : no...\n",
    "\n",
    "### 어떤 원리인건데 그럼??\n",
    "\n",
    "- Maximal Margin Classfier의 일반화된 모델\n",
    "- ???쟤는 또 뭐야???\n",
    "----------\n",
    "- *한정된 시간으로 모든 내용을 다루기는 힘들 수 있지만 일단 스터디 내용은 업로드 합니다. 스터디 발표에서는 책에 나온 내용 위주 & 실제 코드 활용을 통해 진행하려합니다.\n",
    "\n",
    "\n",
    "### Content\n",
    "\n",
    "##### 1. MMC(Maximal Margin Classifier)\n",
    "##### 2. SVC(Support Vector Classifier)\n",
    "##### 3. SVM(Support Vector Machine)\n",
    "##### 4. Extension of SVM(more than two classes)\n",
    "##### 5. Relation between SVM and other statistical model\n",
    "-------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. MMC\n",
    "\n",
    "* Hyperplane\n",
    ">- *def<br />*\n",
    ">a flat affine subspace of hyperplane dimension p − 1.<br /><br />\n",
    ">- ex)<br />\n",
    ">2차원 평면에서는 선이 hyperplane<br /><br />\n",
    ">β0 + β1X1 + β2X2 = 0<br /><br />\n",
    ">로 정의할 수 있음<br /><br />\n",
    "\n",
    "p-dimensional 까지 확장하면...<br /><br />\n",
    "β0 + β1X1 + β2X2 + ... + βpXp = 0  --- (a)<br/><br/>\n",
    "만약 어떤 점 X = (X1, X2,...,Xp)^T 가 식(a)를 만족하면 점 X는 (a) Hyperplane위에 있다.\n",
    "\n",
    "![Alt text](sc01.png)\n",
    "\n",
    "* Hyperplane을 이용해서 분류하기\n",
    "Hyperplane을 이용해서 분류하기 위해 먼저 행렬X(n개의 observations by p개의 dimension)를 정의<br/>\n",
    "\n",
    "![Alt text](sc02.png)\n",
    "\n",
    "위 그룹과 y값 2개로 나뉠수음 train/test 셋으로 나눌수 있기 때문에 lr, tree등 접근 방식이 가능 하지만 여기서는 'separating hyperplane'개념으로 접근\n",
    "\n",
    "#### hyperplane을 통해 관측값들을 그들의 레이블값(y)으로 완벽하게 나눌 수 있다는 컨셉에서 시작\n",
    "\n",
    "![Alt text](sc03.png)\n",
    "\n",
    "이때 각각의 y값에 대해서\n",
    "\n",
    "![Alt text](sc04.png)\n",
    "\n",
    "* The Maximal Margin Classifier\n",
    "\n",
    "#### hyperplane으로 분류를 한다는 것은 알겠는데 그럼 hyperplane은 어떻게 만드나요?\n",
    "\n",
    "-> 경계(margin)간의 거리가 최대(maximal)가 되는 hyperplane을 만들자\n",
    "\n",
    "![Alt text](sc05.png)\n",
    "\n",
    "그럼을 보면 가운데 직선이 maximal margin hyperplane이고 margin은 점선과 직선사이의 수직 거리이다.<br/>\n",
    "margin(점선)은 MMH(직선)의 절편 이동에 의해 만들어지는데 가장 먼저 접하는 관측치까지 이동한다. <br/><br/>\n",
    "이 때, 저 관측치들을 Support Vector라고 한다.(margin 결정을 'support'함 & p-차원 공간 상의 벡터)\n",
    "\n",
    "#### 잘 생각해보면 위 그림에서 margin은 단 3개의 관측치에 의해 결정되는데, 이에 관해서는 나중에 다루어짐\n",
    "\n",
    "* Construction of the Maximal Margin Classifier\n",
    "개념적인 측면에 벗어나서 이제 어떻게 margin을 maximize할 지 최적화 문제로 넘어가자\n",
    "\n",
    "![Alt text](sc06.png)\n",
    "다음 3개의 제약식을 만족하는 최적화 문제\n",
    "즉 M을 값을 최대화하기 위한 β0,...,βp를 찾는 풀이법은 생략...<br/>\n",
    "\n",
    ">#### 한계점 & 해결책\n",
    ">separating hyperplane이 없는 경우 M>0 조건에서 해를 구할 수 없는 경우<br/>\n",
    "![Alt text](sc06.png)\n",
    "위의 그림에서 완벽하게 모든 클래스별로 나누는 hyperplane은 없음...<br/>\n",
    "이럴때는 몇개의 관측치가 error로 들어가더라도 분리해낼 수 있는 모델이 필요하다<br/><br/>\n",
    "-> soft margin이 등장 <br/><br/>\n",
    "MMC의 일반화 형태인 Support vector classifier로 non-separable case를 해결\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ref\n",
    "- An intoduction to statistical learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
